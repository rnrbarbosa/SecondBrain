# Courses
- [ ] [Building Your First ETL Pipeline Using Azure Databricks](https://app.pluralsight.com/library/courses/building-etl-pipeline-microsoft-azure-databricks/table-of-contents) 
- [ ] [Implementing an Azure Databricks Environment in Microsoft Azure](https://app.pluralsight.com/library/courses/microsoft-azure-databricks-environment-implementing)
- [ ] [Getting Started with Delta Lake on Databricks](https://app.pluralsight.com/library/courses/getting-started-delta-lake-databricks)
- [ ] [Working with Azure Databricks Programmatically](https://app.pluralsight.com/library/courses/working-azure-databricks-programmatically)
- [ ] [Managing and Administering the Databricks Service](https://app.pluralsight.com/library/courses/managing-administering-databricks-service)

# Notes

How to deploy Azure Databricks? 1. Create an Azure Databricks workspace in the Azure portal. 
2. Select the type of cluster you want to use, such as an autoscaling cluster or a job-based cluster.
3. Upload your data to the workspace using Azure Blob Storage or Data Lake Storage Gen1/Gen2.
4. Install any necessary libraries and packages required for your project. 
5. Configure your notebook environment and run scripts or notebooks you have created in the workspaceâ€™s notebooks folder. 
6. Monitor the progress of your job or experiment results through the UI or log files stored on DBFS (Databricks File System). 
7. If needed, scale up and down clusters according to your workload requirements with autoscaling enabled
